# backend/app/services/ai_service.py

from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import io
import torch

class AIService:
    def __init__(self):
        print("ğŸ¤– HuggingFace AI ëª¨ë¸(CLIP) ë¡œë”© ì¤‘...")
        # ì—¬ê¸°ê°€ ë°”ë¡œ Hugging Faceì—ì„œ ëª¨ë¸ì„ ê°€ì ¸ì˜¤ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤.
        self.model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        self.processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        print("âœ… ëª¨ë¸ ì¥ì°© ì™„ë£Œ!")

    def image_to_vector(self, image_bytes):
        try:
            # 1. ë°”ì´íŠ¸ í˜•íƒœì˜ ì´ë¯¸ì§€ë¥¼ ì—´ê¸°
            image = Image.open(io.BytesIO(image_bytes))
            
            # 2. AIê°€ ì´í•´í•  ìˆ˜ ìˆê²Œ ë³€í™˜ (ì „ì²˜ë¦¬)
            inputs = self.processor(images=image, return_tensors="pt")
            
            # 3. ë²¡í„° ì¶”ì¶œ (íŠ¹ì§• ë½‘ì•„ë‚´ê¸°)
            with torch.no_grad():
                outputs = self.model.get_image_features(**inputs)
            
            # 4. ê²°ê³¼ë¥¼ íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (DBì— ì €ì¥í•˜ê¸° ìœ„í•´)
            # ì´ ëª¨ë¸ì€ ìˆ«ì 512ê°œì§œë¦¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.
            vector = outputs[0].tolist()
            return vector
            
        except Exception as e:
            print(f"âŒ AI ë³€í™˜ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
            return None

# ì´ ë³€ìˆ˜ë¥¼ ë‹¤ë¥¸ íŒŒì¼ì—ì„œ ê°€ì ¸ë‹¤ ì”ë‹ˆë‹¤
ai_instance = AIService()