# backend/app/services/ai_service.py

from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import io
import torch

class AIService:
    def __init__(self):
        print("ğŸ¤– HuggingFace AI ëª¨ë¸(CLIP) ë¡œë”© ì¤‘...")
        # ì—¬ê¸°ê°€ ë°”ë¡œ Hugging Faceì—ì„œ ëª¨ë¸ì„ ê°€ì ¸ì˜¤ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤.
        self.model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        self.processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        print("âœ… ëª¨ë¸ ì¥ì°© ì™„ë£Œ!")

    def image_to_vector(self, image_bytes):
        try:
            # 1. ë°”ì´íŠ¸ í˜•íƒœì˜ ì´ë¯¸ì§€ë¥¼ ì—´ê¸°
            image = Image.open(io.BytesIO(image_bytes))
            
            # 2. AIê°€ ì´í•´í•  ìˆ˜ ìˆê²Œ ë³€í™˜ (ì „ì²˜ë¦¬)
            inputs = self.processor(images=image, return_tensors="pt")
            
            # 3. ë²¡í„° ì¶”ì¶œ (íŠ¹ì§• ë½‘ì•„ë‚´ê¸°)
            with torch.no_grad():
                outputs = self.model.get_image_features(**inputs)

            # 4. ê²°ê³¼ í…ì„œ ì¶”ì¶œ (transformers ë²„ì „ì— ë”°ë¼ íƒ€ì…ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
            if hasattr(outputs, "pooler_output"):
                image_features = outputs.pooler_output
            elif isinstance(outputs, (tuple, list)):
                image_features = outputs[0]
            else:
                image_features = outputs

            # ì´ ëª¨ë¸ì€ ìˆ«ì 512ê°œì§œë¦¬ ë²¡í„°ë¥¼ ë°˜í™˜
            vector = image_features[0].tolist()
            return vector
            
        except Exception as e:
            print(f"âŒ AI ë³€í™˜ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
            return None

# ì´ ë³€ìˆ˜ë¥¼ ë‹¤ë¥¸ íŒŒì¼ì—ì„œ ê°€ì ¸ë‹¤ ì”ë‹ˆë‹¤
ai_instance = AIService()
